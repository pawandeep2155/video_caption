{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn.functional as F\n",
    "import os \n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import random\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 10\n",
    "n_iter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetInfo():\n",
    "    ####################################################################\n",
    "    # Format of dataset files:\n",
    "    #    1. Resfeat: {id:resnet feature of entire video id as tensor of size ?,1024} \n",
    "    #    2. Caption: {id:['caption1', 'caption2']}\n",
    "    #    3. Video Summary: {id: [[10, 20], [30, 40]]}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.caption = torch.load('../dataset_224/activitynet/caption.pt')\n",
    "        self.summary = torch.load('../dataset_224/activitynet/video_summary.pt')\n",
    "        self.resnet = torch.load('../dataset_224/activitynet/resfeat.pt')\n",
    "        \n",
    "        self.word_to_id_dict = {}\n",
    "        self.id_to_word_dict = {}\n",
    "     \n",
    "    def max_frame_in_action(self):\n",
    "        max_len = 0\n",
    "        for i, (_, value) in enumerate(self.summary.items()):\n",
    "            summary_size = max([x[1] - x[0] + 1 for x in value])\n",
    "            max_len = max(max_len, summary_size)\n",
    "        return max_len \n",
    "        \n",
    "    def max_word_caption(self):\n",
    "        max_len = 0\n",
    "        for i, (_, value) in enumerate(self.caption.items()):\n",
    "            # value is list of caption strings.\n",
    "            caption_sizes = [len(re.findall(' ',x)) for x in value]\n",
    "            caption_len = max(caption_sizes)\n",
    "            max_len = max(max_len, caption_len)\n",
    "        return max_len + 1  # re.findall(' ',str) counts one word less than in str.\n",
    "    \n",
    "    def total_actions(self):\n",
    "        num_actions = 0\n",
    "        for i, (_, value) in enumerate(self.caption.items()):\n",
    "            num_actions += len(value)\n",
    "        return num_actions\n",
    "    \n",
    "    def word_id_dict(self):\n",
    "        self.word_to_id_dict = {\"EOS\":0, \"SOS\":1}\n",
    "        self.id_to_word_dict = { 0:\"EOS\", 1:\"SOS\"}\n",
    "        for i, (_, value) in enumerate(self.caption.items()):\n",
    "            for caption in value:\n",
    "                sentence = caption.split()\n",
    "                for word in sentence:\n",
    "                    if word not in self.word_to_id_dict:\n",
    "                        len_dict = len(self.word_to_id_dict)\n",
    "                        self.word_to_id_dict[word] = len_dict\n",
    "                        self.id_to_word_dict[len_dict] = word\n",
    "                        \n",
    "        return self.word_to_id_dict, self.id_to_word_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset(DatasetInfo):\n",
    "    #############################################################3\n",
    "    # Returns input and output data along with padding.\n",
    "    def __init__(self):\n",
    "        super(MakeDataset, self).__init__()\n",
    "\n",
    "    def action_features(self):\n",
    "        action_list = []\n",
    "        for i, (vid, value) in enumerate(self.resnet.items()):\n",
    "            time_list = self.summary[vid]\n",
    "            for time in time_list:\n",
    "                start_time = time[0]\n",
    "                end_time = time[1]\n",
    "                action_tensor = value[start_time:end_time+1]\n",
    "                action_list.append(action_tensor)\n",
    "        return action_list   \n",
    "    \n",
    "    def input_data(self):\n",
    "        action_list = self.action_features()\n",
    "        input_data = rnn_utils.pad_sequence(action_list, batch_first=True)\n",
    "        return input_data  # tensor size (#actions, max frame in actions, frame_size)\n",
    "        \n",
    "    def output_data(self):\n",
    "        super(MakeDataset, self).word_id_dict()\n",
    "        caption_list = []\n",
    "        for i, (_, value) in enumerate(self.caption.items()):\n",
    "            for caption in value:\n",
    "                caption_ids = [self.word_to_id_dict[x] for x in caption.split()]\n",
    "                caption_ids = [1] + caption_ids  # append SOS\n",
    "                caption_tensor = torch.tensor(caption_ids, dtype=torch.long)\n",
    "                caption_list.append(caption_tensor)\n",
    "                \n",
    "        output_data = rnn_utils.pad_sequence(caption_list, batch_first=True)  # append EOS \n",
    "        \n",
    "        return output_data # tensor size (#actions, max word in caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_loader(Dataset):\n",
    "    ###############################################################\n",
    "    # Returns input, output data per batch \n",
    "    def __init__(self, data_size, makedataset):\n",
    "        self.data_size = data_size\n",
    "        self.input_data = makedataset.input_data()\n",
    "        self.output_data = makedataset.output_data()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx], self.output_data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bidirectional):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = 1\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.Lstm = nn.LSTM(input_size = self.input_size, hidden_size=self.hidden_size, \n",
    "                            num_layers=self.num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "    \n",
    "    def forward(self, input_state, hidden_state):\n",
    "        output, hidden = self.Lstm(input_state, hidden_state)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        h0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size)\n",
    "        c0 = h0\n",
    "        return (h0, c0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size, bidirectional):\n",
    "        \n",
    "        super(AttnDecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dec_output_size = input_size\n",
    "        self.embedding = nn.Embedding(self.dec_output_size, self.embedding_size)\n",
    "        self.num_layers = 1\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.Lstm = nn.LSTM(input_size = self.embedding_size, \n",
    "                            hidden_size=self.hidden_size, \n",
    "                            num_layers=self.num_layers, batch_first=True, \n",
    "                            bidirectional=bidirectional)\n",
    "        self.output_layer = nn.Linear(self.num_directions*self.hidden_size, self.dec_output_size)\n",
    "        \n",
    "    def forward(self, input_state, hidden):\n",
    "       # print('input state', input_state.size())\n",
    "        embedded = self.embedding(input_state)\n",
    "        #print('embedded', embedded.size())\n",
    "        output, hidden = self.Lstm(embedded, hidden)\n",
    "        #print('output', output.size())\n",
    "        output = F.softmax(self.output_layer(output), dim=2)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, output_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    #print('train started')\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    #print('encoder hidden', encoder_hidden[0].size())\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "  \n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    #print('encoder output', encoder_output.size())\n",
    "    \n",
    "    decoder_output, decoder_hidden = decoder(output_tensor, decoder_hidden)\n",
    "    \n",
    "    #print('decoder otuput', decoder_output.size())\n",
    "    \n",
    "    loss = criterion(decoder_output.view(-1, decoder_output.size(2)), output_tensor.view(-1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, dataloader, n_iters=1, print_every=1000, \n",
    "               plot_every=100, learning_rate=0.01):\n",
    "    \n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for idx, (input_tensor, output_tensor) in enumerate(dataloader):\n",
    "            print('Id ', idx, end=' ')\n",
    "            #print('input', input_tensor.size())\n",
    "            #print('output', output_tensor.size())\n",
    "            loss = train(input_tensor, output_tensor, encoder, decoder, encoder_optimizer,\n",
    "                         decoder_optimizer, criterion)\n",
    "            print('loss', loss)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.switch_backend('agg')\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    info = DatasetInfo()\n",
    "    data_size = info.total_actions()\n",
    "    word2id_dict, id2word_dict = info.word_id_dict()\n",
    "\n",
    "    make = MakeDataset()\n",
    "    loader = Dataset_loader(data_size, make)\n",
    "    dataloader = DataLoader(loader, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "    # encoder dimensions\n",
    "    enc_input_size = 1024\n",
    "    enc_hidden_size = 256\n",
    "    \n",
    "    bidirectional = True\n",
    "    num_directions = 2 if bidirectional else 1\n",
    "    \n",
    "    # decoder dimensions\n",
    "    dec_input_size = len(word2id_dict)\n",
    "    dec_hidden_size = enc_hidden_size\n",
    "    embedding_size = 100\n",
    "\n",
    "    encoder = EncoderLSTM(enc_input_size, enc_hidden_size, bidirectional)\n",
    "    decoder = AttnDecoderLSTM(dec_input_size, dec_hidden_size, embedding_size,bidirectional)\n",
    "\n",
    "    trainIters(encoder, decoder, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('../dataset_224/activitynet/resfeat.pt')\n",
    "d = {}\n",
    "for i, (key, value) in enumerate(a.items()):\n",
    "    b = random.randint(760,800)\n",
    "    array = torch.randn((b, 1024))\n",
    "    d[key] = array\n",
    "torch.save(d,''../dataset_224/activitynet/resfeat.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = torch.load('../dataset_224/activitynet/video_summary.pt')\n",
    "# c = torch.load('../dataset_224/activitynet/caption.pt')\n",
    "\n",
    "# d = {}\n",
    "# for i, (key, value) in enumerate(s.items()):\n",
    "#     time = [value[0]]\n",
    "#     d[key] = time\n",
    "\n",
    "# torch.save(d, '../dataset_224/activitynet/video_summary.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a ={\"hello\":0, \"how\":1}\n",
    "# b = \"hello how how hello\"\n",
    "# embedded = nn.Embedding(2,5)\n",
    "# ten1 = torch.tensor([0,1,1,0], dtype=torch.long)\n",
    "# ten2 = torch.tensor([1,1,0,0], dtype=torch.long)\n",
    "# ten3 = torch.tensor([0,1,0,0], dtype=torch.long)\n",
    "# e = list(np.asarray(ten1))\n",
    "# print(type(e))\n",
    "# tens = torch.tensor([list(np.asarray(ten1)), list(np.asarray(ten2)), list(np.asarray(ten1))], dtype=torch.long)\n",
    "\n",
    "# d = embedded(tens)\n",
    "# print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     for index in range(num_actions):\n",
    "    \n",
    "#         for ei in range(input_length):\n",
    "#             encoder_output, encoder_hidden = encoder(\n",
    "#                 input_tensor[index][ei], encoder_hidden)\n",
    "#         encoder_outputs = encoder_output[0, 0]\n",
    "\n",
    "#         decoder_input = torch.from_numpy(word_to_index[\"SOS\"]).to(device)\n",
    "\n",
    "#         decoder_hidden = encoder_hidden\n",
    "\n",
    "#         use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "#         if use_teacher_forcing:\n",
    "#             # Teacher forcing: Feed the target as the next input\n",
    "#             for di in range(target_length):\n",
    "#                 decoder_output, decoder_hidden = decoder(\n",
    "#                     decoder_input, decoder_hidden)\n",
    "#                 loss += criterion(decoder_output, target_tensor[index][di])\n",
    "#                 decoder_input = target_tensor[index][di]  # Teacher forcing\n",
    "\n",
    "#         else:\n",
    "#             # Without teacher forcing: use its own predictions as the next input\n",
    "#             for di in range(target_length):\n",
    "#                 decoder_output, decoder_hidden = decoder(\n",
    "#                     decoder_input, decoder_hidden)\n",
    "#                 topv, topi = decoder_output.topk(1)\n",
    "#                 decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "#                 loss += criterion(decoder_output, target_tensor[di])\n",
    "#                 if decoder_input.item() == EOS_token:\n",
    "#                     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         for i, (input_tensor, output_tensor) in enumerate(dataloader):\n",
    "#             print('i', i)\n",
    "#             loss = train(input_tensor, output_tensor, encoder,\n",
    "#                          decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "#             print_loss_total += loss\n",
    "#             plot_loss_total += loss\n",
    "\n",
    "#         if iter % print_every == 0:\n",
    "#             print_loss_avg = print_loss_total / print_every\n",
    "#             print_loss_total = 0\n",
    "#             print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "#                                          iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "#         if iter % plot_every == 0:\n",
    "#             plot_loss_avg = plot_loss_total / plot_every\n",
    "#             plot_losses.append(plot_loss_avg)\n",
    "#             plot_loss_total = 0\n",
    "\n",
    "#     showPlot(plot_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
