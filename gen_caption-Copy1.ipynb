{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import os \n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import random\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 10\n",
    "n_iter = 5\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    }
   ],
   "source": [
    "class DatasetInfo():\n",
    "    ####################################################################\n",
    "    # Format of dataset files:\n",
    "    #    1. Resfeat: {id:resnet feature of entire video id as tensor of size ?,1024} \n",
    "    #    2. Caption: {id:['caption1', 'caption2']}\n",
    "    #    3. Video Summary: {id: [[10, 20], [30, 40]]}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.caption = torch.load('../dataset_224/activitynet/caption.pt')\n",
    "        self.summary = torch.load('../dataset_224/activitynet/video_summary.pt')\n",
    "        self.resnet = torch.load('../dataset_224/activitynet/resfeat.pt')\n",
    "     \n",
    "    def max_frame_in_action(self):\n",
    "        max_len = 0\n",
    "        for i, (_, value) in enumerate(self.summary.items()):\n",
    "            summary_size = max([x[1] - x[0] + 1 for x in value])\n",
    "            max_len = max(max_len, summary_size)\n",
    "        return max_len \n",
    "        \n",
    "    def max_word_caption(self):\n",
    "        max_len = 0\n",
    "        for i, (_, value) in enumerate(self.caption.items()):\n",
    "            # value is list of caption strings.\n",
    "            caption_sizes = [len(re.findall(' ',x)) for x in value]\n",
    "            caption_len = max(caption_sizes)\n",
    "            max_len = max(max_len, caption_len)\n",
    "        return max_len + 1  # re.findall(' ',str) counts one word less than in str.\n",
    "    \n",
    "    def total_actions(self):\n",
    "        num_actions = 0\n",
    "        for i, (_, value) in enumerate(self.caption.items()):\n",
    "            num_actions += len(value)\n",
    "        return num_actions\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        vocab = set()\n",
    "        for i,(_, value) in enumerate(self.caption.items()):\n",
    "            for caption in value:\n",
    "                sentence = caption.split()\n",
    "                for word in sentence:\n",
    "                    if word not in vocab:\n",
    "                        vocab.add(word)\n",
    "        return len(vocab) + 2  # one for SOS, other for EOS \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "class MakeDataset(DatasetInfo):\n",
    "    def __init__(self, num_actions, max_frame_action, frame_size, max_word_caption, vocab_size):\n",
    "        super(MakeDataset, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.max_frame_action = max_frame_action\n",
    "        self.frame_size = frame_size\n",
    "        self.max_word_caption = max_word_caption\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_to_id_dict = {}\n",
    "        self.id_to_word_dict = {}\n",
    "    \n",
    "    \n",
    "    def word_id_dict(self):\n",
    "        for i, (_, value) in enumerate(self.caption.items()):\n",
    "            for caption in value:\n",
    "                sentence = caption.split()\n",
    "                for word in sentence:\n",
    "                    if word not in word_to_id_dict:\n",
    "                        len_dict = len(word_to_id_dict)\n",
    "                        self.word_to_id_dict[word] = len_dict\n",
    "                        self.id_to_word_dict[len_dict] = word\n",
    "                        \n",
    "    \n",
    "    def input_data(self):\n",
    "        input_tensor = torch.zeros(self.num_actions, self.max_frame_action, self.frame_size)\n",
    "        for i, (_, value) in enumerate(self.resnet):\n",
    "                #input_tensor[i] = value with padded sequ\n",
    "        \n",
    "        \n",
    "         \n",
    "        \n",
    "    def output_data(self):\n",
    "        pass\n",
    "    \n",
    "a = MakeDataset(1,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "class A:\n",
    "    def hello(self):\n",
    "        print(global a)\n",
    "a = A()\n",
    "a.hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset vocab size\n",
    "dataset = torch.load(dataset_path)\n",
    "data_set = set()\n",
    "for data_kind in dataset:\n",
    "    for _, (_, value) in enumerate(dataset[data_kind].items()):\n",
    "        captions = list(value.keys())\n",
    "        for caption in captions:\n",
    "            tokens = caption.split()\n",
    "            for token in tokens:\n",
    "                data_set.add(token)\n",
    "\n",
    "dict_size = len(data_set) + 2 # binary array size for each unique word in dataset, EOS, SOS token.\n",
    "\n",
    "# create dictionary for tokens in dataset\n",
    "sos_array = np.eye(dict_size, dtype=np.int)[SOS_token] # word_to_index dictionary value\n",
    "eos_array = np.eye(dict_size, dtype=np.int)[EOS_token]  # word_to_index dictionary value\n",
    "\n",
    "sos_array_str = np.array2string(sos_array)  # index_to_word dictionary key.\n",
    "eos_array_str = np.array2string(eos_array)  # index_to_word dictionary key. \n",
    "\n",
    "word_to_index = {\"SOS\":sos_array, \"EOS\":eos_array}\n",
    "index_to_word = {sos_array_str:\"SOS\", eos_array_str:\"EOS\"}\n",
    "for i, token in enumerate(data_set):\n",
    "    one_hot_vector = np.eye(dict_size, dtype=np.int)[i]\n",
    "    word_to_index[token] = one_hot_vector\n",
    "    \n",
    "    one_hot_str = np.array2string(one_hot_vector)\n",
    "    index_to_word[one_hot_str] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_feat = torch.load(resfeat_path) # resnet_feat is dict with key:id, value: torch array.\n",
    "video_ids = list(resnet_feat.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.BLstm = nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=2, bidirectional=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.BLstm(input, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size), torch.zeros(1, 1, self.hidden_size))\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.BLstm = nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=2, bidirectional=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.BLstm(input, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_loader(Dataset):\n",
    "\n",
    "    \n",
    "    def __init__(self, data_kind):\n",
    "        self.data_kind = data_kind  # train/valid/test\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(resnet_feat)\n",
    "  \n",
    "    def word_to_array(self, word):\n",
    "        return word_to_index[word]\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        batch_start = idx*batch_size\n",
    "        batch_end = min(batch_size*(idx+1), len(resnet_feat))\n",
    "        vids = video_ids[batch_start:batch_end]\n",
    "        \n",
    "        # Get input \n",
    "        input = []\n",
    "        for vid in vids:\n",
    "            print(vid)\n",
    "            value = dataset[self.data_kind][vid]\n",
    "            time_list = list(value.values())\n",
    "            for time in time_list:\n",
    "                start_time = int(time[0])\n",
    "                end_time = int(time[1])\n",
    "                input.append(resnet_feat[vid][start_time:end_time])\n",
    "                \n",
    "        # Add zero padding \n",
    "        max_len = max([x.shape[0] for x in input])\n",
    "        tensor = torch.zeros(len(input), max_len, input[0].shape[1])\n",
    "        \n",
    "        for i, item in enumerate(input):\n",
    "            pack_array = np.zeros([max_len - item.shape[0], input[0].shape[1]], dtype=np.int8)\n",
    "            item = np.concatenate((item,pack_array), axis=0)\n",
    "            tensor[i] = torch.from_numpy(item)\n",
    "                \n",
    "        input_tensor = tensor\n",
    "        \n",
    "        # Get output\n",
    "        output = []\n",
    "        for vid in vids:\n",
    "            value = dataset[self.data_kind][vid]\n",
    "            for caption in value: # each sentence\n",
    "                cap_array = np.zeros((len(caption),dict_size), dtype=np.int8)    \n",
    "                words = caption.split()\n",
    "                for i, word in enumerate(words):  # each word\n",
    "                    word_arr = word_to_index[word]\n",
    "                    cap_array[i] = word_arr\n",
    "                output.append(cap_array)\n",
    "        \n",
    "        # Add zero padding \n",
    "        max_len = max([x.shape[0] for x in output])\n",
    "        tensor = torch.zeros(len(output), max_len, output[0].shape[1])\n",
    "        \n",
    "        for i, item in enumerate(output):\n",
    "            pack_array = np.zeros([max_len - item.shape[0], output[0].shape[1]])\n",
    "            item = np.concatenate((item,pack_array), axis=0)\n",
    "            tensor[i] = torch.from_numpy(item)\n",
    "        \n",
    "        output_tensor = tensor\n",
    "        \n",
    "        return input_tensor, output_tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    print('train started')\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "  \n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "    \n",
    "    loss = 0\n",
    "    decoder_input = torch.from_numpy(word_to_index[\"SOS\"])\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_output, _ = decoder(decoder_input, target_tensor)\n",
    "    \n",
    "    loss = criterion(decoder_output, target_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()  # loss for batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.switch_backend('agg')\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Dataset_loader(data_kind='train')\n",
    "dataloader = DataLoader(loader, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        print('iter', iter)\n",
    "        input_t, output_t = loader[0]\n",
    "        loss = train(input_t, output_t, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        \n",
    "#         for i, (input_tensor, output_tensor) in enumerate(dataloader):\n",
    "#             print('i', i)\n",
    "#             loss = train(input_tensor, output_tensor, encoder,\n",
    "#                          decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "#             print_loss_total += loss\n",
    "#             plot_loss_total += loss\n",
    "\n",
    "#         if iter % print_every == 0:\n",
    "#             print_loss_avg = print_loss_total / print_every\n",
    "#             print_loss_total = 0\n",
    "#             print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "#                                          iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "#         if iter % plot_every == 0:\n",
    "#             plot_loss_avg = plot_loss_total / plot_every\n",
    "#             plot_losses.append(plot_loss_avg)\n",
    "#             plot_loss_total = 0\n",
    "\n",
    "#     showPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_size = 2048\n",
    "\n",
    "dec_input_size = dict_size\n",
    "dec_output_size = dict_size\n",
    "\n",
    "dec_hidden_size = dec_output_size\n",
    "enc_hidden_size = dec_hidden_size\n",
    "enc_output_size = dec_hidden_size\n",
    "\n",
    "encoder1 = EncoderRNN(enc_input_size, enc_hidden_size, enc_output_size)\n",
    "decoder1 = DecoderRNN(dec_input_size, dec_hidden_size, dec_output_size)\n",
    "\n",
    "trainIters(encoder1, decoder1, 1, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     for index in range(num_actions):\n",
    "    \n",
    "#         for ei in range(input_length):\n",
    "#             encoder_output, encoder_hidden = encoder(\n",
    "#                 input_tensor[index][ei], encoder_hidden)\n",
    "#         encoder_outputs = encoder_output[0, 0]\n",
    "\n",
    "#         decoder_input = torch.from_numpy(word_to_index[\"SOS\"]).to(device)\n",
    "\n",
    "#         decoder_hidden = encoder_hidden\n",
    "\n",
    "#         use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "#         if use_teacher_forcing:\n",
    "#             # Teacher forcing: Feed the target as the next input\n",
    "#             for di in range(target_length):\n",
    "#                 decoder_output, decoder_hidden = decoder(\n",
    "#                     decoder_input, decoder_hidden)\n",
    "#                 loss += criterion(decoder_output, target_tensor[index][di])\n",
    "#                 decoder_input = target_tensor[index][di]  # Teacher forcing\n",
    "\n",
    "#         else:\n",
    "#             # Without teacher forcing: use its own predictions as the next input\n",
    "#             for di in range(target_length):\n",
    "#                 decoder_output, decoder_hidden = decoder(\n",
    "#                     decoder_input, decoder_hidden)\n",
    "#                 topv, topi = decoder_output.topk(1)\n",
    "#                 decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "#                 loss += criterion(decoder_output, target_tensor[di])\n",
    "#                 if decoder_input.item() == EOS_token:\n",
    "#                     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
